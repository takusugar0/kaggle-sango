{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom tqdm import tqdm\nimport sys\nimport numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image\nimport ast\nsys.path.append('../input/tensorflow-great-barrier-reef')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-27T05:52:46.293444Z","iopub.execute_input":"2022-01-27T05:52:46.293771Z","iopub.status.idle":"2022-01-27T05:52:48.084270Z","shell.execute_reply.started":"2022-01-27T05:52:46.293738Z","shell.execute_reply":"2022-01-27T05:52:48.083221Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/bytetrack /kaggle/working/tmp/","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:48.086220Z","iopub.execute_input":"2022-01-27T05:52:48.086567Z","iopub.status.idle":"2022-01-27T05:52:55.312452Z","shell.execute_reply.started":"2022-01-27T05:52:48.086505Z","shell.execute_reply":"2022-01-27T05:52:55.311239Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/working/tmp/cython_bbox-0.1.3/cython_bbox-0.1.3\n!pip install /kaggle/working/tmp/lap-0.4.0/lap-0.4.0\n!pip install /kaggle/working/tmp/loguru-0.5.3-py3-none-any.whl\n!pip install /kaggle/working/tmp/ninja-1.10.2.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl\n!pip install /kaggle/working/tmp/thop-0.0.31.post2005241907-py3-none-any.whl\n!pip install /kaggle/working/tmp/pycocotools-2.0.2/dist/pycocotools-2.0.2.tar\n\n!pip install /kaggle/working/tmp/onnx-1.8.0-cp37-cp37m-manylinux2010_x86_64.whl\n!pip install /kaggle/working/tmp/onnxoptimizer-0.2.6-cp37-cp37m-manylinux2014_x86_64.whl\n!pip install /kaggle/working/tmp/onnx-simplifier-0.3.5/onnx-simplifier-0.3.5\n\n!pip install /kaggle/working/tmp/pycocotools-2.0.2/dist/pycocotools-2.0.2.tar\n\n\n%cd /kaggle/working\n!cp -r ../input/bytetrack/ByteTrack /kaggle/working/\n%cd /kaggle/working/ByteTrack\n!pip install -e . --no-deps\n%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:55.315154Z","iopub.execute_input":"2022-01-27T05:52:55.315386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('../input/bytetrack/ByteTrack')","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.372136Z","iopub.status.idle":"2022-01-27T05:52:31.372910Z","shell.execute_reply.started":"2022-01-27T05:52:31.372604Z","shell.execute_reply":"2022-01-27T05:52:31.372645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ByteTrack.yolox.tracker.byte_tracker import BYTETracker","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.374429Z","iopub.status.idle":"2022-01-27T05:52:31.375276Z","shell.execute_reply.started":"2022-01-27T05:52:31.374992Z","shell.execute_reply":"2022-01-27T05:52:31.375021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONF      = 0.01\nIOU       = 0.20\nIMG_SIZE  = 3600\nAUGMENT   = True\nCKPT_PATH = '../input/reef-baseline-fold12/l6_3600_uflip_vm5_f12_up/f1/best.pt'","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.383724Z","iopub.status.idle":"2022-01-27T05:52:31.384575Z","shell.execute_reply.started":"2022-01-27T05:52:31.384288Z","shell.execute_reply":"2022-01-27T05:52:31.384317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.386488Z","iopub.status.idle":"2022-01-27T05:52:31.387298Z","shell.execute_reply.started":"2022-01-27T05:52:31.387040Z","shell.execute_reply":"2022-01-27T05:52:31.387068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def get_path(row):\n    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    return row","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.388840Z","iopub.status.idle":"2022-01-27T05:52:31.389619Z","shell.execute_reply.started":"2022-01-27T05:52:31.389301Z","shell.execute_reply":"2022-01-27T05:52:31.389329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.391162Z","iopub.status.idle":"2022-01-27T05:52:31.391956Z","shell.execute_reply.started":"2022-01-27T05:52:31.391681Z","shell.execute_reply":"2022-01-27T05:52:31.391713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.399508Z","iopub.execute_input":"2022-01-27T05:52:31.400150Z","iopub.status.idle":"2022-01-27T05:52:31.433921Z","shell.execute_reply.started":"2022-01-27T05:52:31.400104Z","shell.execute_reply":"2022-01-27T05:52:31.432729Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.435391Z","iopub.status.idle":"2022-01-27T05:52:31.436129Z","shell.execute_reply.started":"2022-01-27T05:52:31.435861Z","shell.execute_reply":"2022-01-27T05:52:31.435888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.01, iou=0.50):\n    model = torch.hub.load('../input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.437774Z","iopub.status.idle":"2022-01-27T05:52:31.438645Z","shell.execute_reply.started":"2022-01-27T05:52:31.438293Z","shell.execute_reply":"2022-01-27T05:52:31.438322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = []\n    detects = []\n    if results.pandas().xyxy[0].shape[0] == 0:\n        return [],[]\n    else:\n        for idx, row in results.pandas().xyxy[0].iterrows():\n            if row.confidence > 0.01:\n                detects.append([int(row.xmin), int(row.ymin), int(row.xmax), int(row.ymax), row.confidence])\n                preds.append([row.xmin, row.ymin, row.xmax-row.xmin, row.ymax-row.ymin])\n                #     bboxes  = preds[['xmin','ymin','xmax','ymax', 'confidence']].values.astype(int)\n#         bboxes  = voc2coco(bboxes,height,width).astype(int)\n#         confs   = preds.confidence.values\n        return detects, preds\n        \n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.562003Z","iopub.execute_input":"2022-01-27T05:52:31.562346Z","iopub.status.idle":"2022-01-27T05:52:31.588799Z","shell.execute_reply.started":"2022-01-27T05:52:31.562305Z","shell.execute_reply":"2022-01-27T05:52:31.587458Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.594939Z","iopub.execute_input":"2022-01-27T05:52:31.598271Z","iopub.status.idle":"2022-01-27T05:52:31.610777Z","shell.execute_reply.started":"2022-01-27T05:52:31.598225Z","shell.execute_reply":"2022-01-27T05:52:31.609607Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n# image_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\nimage_paths = df[df.sequence == 53708].image_path.tolist()\nfor idx, path in enumerate(image_paths):\n#     break\n    img = cv2.imread(path)[...,::-1]\n    detects, preds = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n    print(len(preds))\n#     display(show_img(img, preds, bbox_format='coco'))\n    if idx>15:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.616142Z","iopub.execute_input":"2022-01-27T05:52:31.618518Z","iopub.status.idle":"2022-01-27T05:52:31.649041Z","shell.execute_reply.started":"2022-01-27T05:52:31.618474Z","shell.execute_reply":"2022-01-27T05:52:31.646498Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#######################################################\n#                      Tracking                       #\n#######################################################\n\n# Tracker will update tracks based on detections from current frame\n# Matching based on euclidean distance between bbox centers of detections \n# from current frame and tracked_objects based on previous frames\n# You can check it's parameters in norfair docs\n# https://github.com/tryolabs/norfair/blob/master/docs/README.md\n# tracker = Tracker(\n#     distance_function=euclidean_distance, \n#     distance_threshold=30,\n#     hit_inertia_min=3,\n#     hit_inertia_max=6,\n#     initialization_delay=1,\n# )\nclass args:\n    det_thresh = 0.011\n    track_thresh = 0.001\n    track_buffer = 30\n    mot20 = False\n    match_thresh = 0.001\n#     aspect_ratio_thresh = 1.6\n    min_box_area = 1000\n    \nfrom yolox.tracker.byte_tracker import BYTETracker\ntracker = BYTETracker(args)\n\n# Save frame_id into detection to know which tracks have no detections on current frame\nframe_id = 0\n#######################################################","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:31.652365Z","iopub.status.idle":"2022-01-27T05:52:31.655033Z","shell.execute_reply.started":"2022-01-27T05:52:31.654723Z","shell.execute_reply":"2022-01-27T05:52:31.654756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n# image_paths = df[df.num_bbox>1].sample(100).image_path.tolist()\n# image_paths = df[100:200].image_path.tolist()\nimage_paths = df[df.sequence == 53708].image_path.tolist()\nfor idx, path in enumerate(image_paths):\n    img = cv2.imread(path)[...,::-1]\n    height, width = img.shape[:2]\n    detects, preds = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n#     r = model(img, size=10000, augment=False)\n    #######################################################\n    #                      Tracking                       #\n    #######################################################\n    preds = [] # if you want to visualize bboxes detected with bytetrack(bytetrackでの追跡のみ表示したくなければコメントアウトする)\n    # Update tracks using detects from current frame\n    if len(detects):\n        tracked_objects = tracker.update(np.array(detects), [height, width], np.array([IMG_SIZE, IMG_SIZE]))\n#         print(len(tracked_objects))\n#         print(\"detects: {}, tracked_object: {}\".format(len(detects), len(tracked_objects)))\n        for tobj in tracked_objects:\n            # Add objects that have no detections on current frame to predictions\n            tlwh = tobj.tlwh\n            if tlwh[2] * tlwh[3] > args.min_box_area:\n                x_min = int(tlwh[0])\n                y_min = int(tlwh[1])\n                bbox_width = int(tlwh[2])\n                bbox_height = int(tlwh[3])\n                preds.append([x_min, y_min, bbox_width, bbox_height])\n                score = tobj.score\n                print('{} {:.2f} {} {} {} {}'.format(frame_id, score, x_min, y_min, bbox_width, bbox_height))\n    #         preds.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        #######################################################\n                display(show_img(img, preds, bbox_format='coco'))\n    if idx>100:\n        break\n    frame_id +=1","metadata":{"execution":{"iopub.status.busy":"2022-01-27T05:52:33.575182Z","iopub.execute_input":"2022-01-27T05:52:33.575474Z","iopub.status.idle":"2022-01-27T05:52:33.597790Z","shell.execute_reply.started":"2022-01-27T05:52:33.575443Z","shell.execute_reply":"2022-01-27T05:52:33.596493Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"detects","metadata":{"execution":{"iopub.status.busy":"2022-01-26T07:28:12.4787Z","iopub.execute_input":"2022-01-26T07:28:12.478978Z","iopub.status.idle":"2022-01-26T07:28:12.484493Z","shell.execute_reply.started":"2022-01-26T07:28:12.478945Z","shell.execute_reply":"2022-01-26T07:28:12.48367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracked_objects = tracker.update(np.array(detects), [height, width], np.array([IMG_SIZE, IMG_SIZE]))","metadata":{"execution":{"iopub.status.busy":"2022-01-26T07:58:19.468495Z","iopub.execute_input":"2022-01-26T07:58:19.469198Z","iopub.status.idle":"2022-01-26T07:58:19.47664Z","shell.execute_reply.started":"2022-01-26T07:58:19.469159Z","shell.execute_reply":"2022-01-26T07:58:19.475879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker.info","metadata":{"execution":{"iopub.status.busy":"2022-01-26T08:00:59.475278Z","iopub.execute_input":"2022-01-26T08:00:59.475825Z","iopub.status.idle":"2022-01-26T08:00:59.499462Z","shell.execute_reply.started":"2022-01-26T08:00:59.475789Z","shell.execute_reply":"2022-01-26T08:00:59.49857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-01-26T01:59:11.101959Z","iopub.execute_input":"2022-01-26T01:59:11.10226Z","iopub.status.idle":"2022-01-26T01:59:11.130982Z","shell.execute_reply.started":"2022-01-26T01:59:11.102202Z","shell.execute_reply":"2022-01-26T01:59:11.130353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.hub.load('../input/yolov5-lib-ds', \n                       'custom', \n                       path='../input/reef-baseline-fold12/l6_3600_uflip_vm5_f12_up/f1/best.pt',\n                       source='local',\n                       force_reload=True)  # local repo\nmodel.conf = 0.01","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:00:11.72703Z","iopub.execute_input":"2022-01-24T13:00:11.727734Z","iopub.status.idle":"2022-01-24T13:00:12.381044Z","shell.execute_reply.started":"2022-01-24T13:00:11.727699Z","shell.execute_reply":"2022-01-24T13:00:12.38029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    anno = ''\n    height, width = img.shape[0], img.shape[1]\n    r = model(img, size=IMG_SIZE, augment=True)\n    if r.pandas().xyxy[0].shape[0] == 0:\n        anno = ''\n    else:\n        for idx, row in r.pandas().xyxy[0].iterrows():\n            if row.confidence > 0.15:\n                anno += '{} {} {} {} {} '.format(row.confidence, int(row.xmin), int(row.ymin), int(row.xmax-row.xmin), int(row.ymax-row.ymin))\n#                 pred.append([row.confidence, row.xmin, row.ymin, row.xmax-row.xmin, row.ymax-row.ymin])\n                detects.append([int(row.xmin), int(row.ymin), int(row.xmax), int(row.ymax), row.confidence])\n    # Update tracks using detects from current frame\n    if len(detects):\n        tracked_objects = tracker.update(np.array(detects), np.array([height, width, frame_id]), np.array([IMG_SIZE, IMG_SIZE]))\n        for tobj in tracked_objects:\n            # Add objects that have no detections on current frame to predictions\n            tlwh = tobj.tlwh\n            vertical = tlwh[2] / tlwh[3] > 1.6\n            if tlwh[2] * tlwh[3] > args.min_box_area and not vertical:\n                x_min = int(tlwh[0])\n                y_min = int(tlwh[1])\n                bbox_width = int(tlwh[2])\n                bbox_height = int(tlwh[3])\n#                 preds.append([x_min, y_min, bbox_width, bbox_height])\n                score = tobj.score\n                anno += '{} {} {} {} {} '.format(score, x_min, y_min, bbox_width, bbox_height)\n    #         preds.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        #######################################################    \n            \n    pred_df['annotations'] = anno.strip(' ')\n    env.predict(pred_df)\n    print('Prediction:', anno.strip(' '))\n    frame_id += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-24T13:00:13.020792Z","iopub.execute_input":"2022-01-24T13:00:13.021303Z","iopub.status.idle":"2022-01-24T13:00:16.711355Z","shell.execute_reply.started":"2022-01-24T13:00:13.021262Z","shell.execute_reply":"2022-01-24T13:00:16.710592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}